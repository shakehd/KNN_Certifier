"""
This type stub file was generated by pyright.
"""

import multiprocessing
from typing import Any, Callable, Optional
from pebble.pool.base_pool import BasePool, PoolContext, ProcessMapFuture, Task, Worker
from pebble.pool.channel import WorkerChannel
from pebble.common import ProcessFuture, Result

class ProcessPool(BasePool):
    """Allows to schedule jobs within a Pool of Processes.

    max_workers is an integer representing the amount of desired process workers
    managed by the pool.
    If max_tasks is a number greater than zero,
    each worker will be restarted after performing an equal amount of tasks.

    initializer must be callable, if passed, it will be called
    every time a worker is started, receiving initargs as arguments.

    """
    def __init__(self, max_workers: int = ..., max_tasks: int = ..., initializer: Callable = ..., initargs: list = ..., context: multiprocessing.context.BaseContext = ...) -> None:
        ...
    
    def schedule(self, function: Callable, args: list = ..., kwargs: dict = ..., timeout: float = ...) -> ProcessFuture:
        """Schedules *function* to be run the Pool.

        *args* and *kwargs* will be forwareded to the scheduled function
        respectively as arguments and keyword arguments.

        *timeout* is an integer, if expires the task will be terminated
        and *Future.result()* will raise *TimeoutError*.

        A *pebble.ProcessFuture* object is returned.

        """
        ...
    
    def submit(self, function: Callable, timeout: Optional[float], *args, **kwargs) -> ProcessFuture:
        """This function is provided for compatibility with
        `asyncio.loop.run_in_executor`.

        For scheduling jobs within the pool use `schedule` instead.

        """
        ...
    
    def map(self, function: Callable, *iterables, **kwargs) -> ProcessMapFuture:
        """Computes the *function* using arguments from
        each of the iterables. Stops when the shortest iterable is exhausted.

        *timeout* is an integer, if expires the task will be terminated
        and the call to next will raise *TimeoutError*.
        The *timeout* is applied to each chunk of the iterable.

        *chunksize* controls the size of the chunks the iterable will
        be broken into before being passed to the function.

        A *pebble.ProcessFuture* object is returned.

        """
        ...
    


def task_scheduler_loop(pool_manager: PoolManager): # -> None:
    ...

def pool_manager_loop(pool_manager: PoolManager): # -> None:
    ...

def message_manager_loop(pool_manager: PoolManager): # -> None:
    ...

class PoolManager:
    """Combines Task and Worker Managers providing a higher level one."""
    def __init__(self, context: PoolContext, mp_context: multiprocessing.context.BaseContext) -> None:
        ...
    
    def start(self): # -> None:
        ...
    
    def stop(self): # -> None:
        ...
    
    def schedule(self, task: Task): # -> None:
        """Schedules a new Task in the PoolManager."""
        ...
    
    def process_next_message(self, timeout: float): # -> None:
        """Processes the next message coming from the workers."""
        ...
    
    def update_status(self): # -> None:
        ...
    
    def update_tasks(self): # -> None:
        """Handles timing out Tasks."""
        ...
    
    def update_workers(self): # -> None:
        """Handles unexpected processes termination."""
        ...
    
    def handle_worker_expiration(self, expiration: tuple): # -> None:
        ...
    
    def find_expired_task(self, worker_id: int) -> Task:
        ...
    


class TaskManager:
    """Manages the tasks flow within the Pool.

    Tasks are registered, acknowledged and completed.
    Timing out and cancelled tasks are handled as well.
    """
    def __init__(self, task_done_callback: Callable) -> None:
        ...
    
    def register(self, task: Task): # -> None:
        ...
    
    def task_start(self, task_id: int, worker_id: Optional[int]): # -> None:
        ...
    
    def task_done(self, task_id: int, result: Result): # -> None:
        """Set the tasks result and run the callback."""
        ...
    
    def task_problem(self, task_id: int, error: Exception): # -> None:
        """Set the task with the error it caused within the Pool."""
        ...
    
    def timeout_tasks(self) -> tuple:
        ...
    
    def cancelled_tasks(self) -> tuple:
        ...
    
    @staticmethod
    def timeout(task: Task) -> bool:
        ...
    


class WorkerManager:
    """Manages the workers related mechanics within the Pool.

    Maintains the workers active and encapsulates their communication logic.
    """
    def __init__(self, workers: int, worker_parameters: Worker, mp_context: multiprocessing.context.BaseContext) -> None:
        ...
    
    def dispatch(self, task: Task): # -> None:
        ...
    
    def receive(self, timeout: float): # -> NoMessage:
        ...
    
    def inspect_workers(self) -> tuple:
        """Updates the workers status.

        Returns the workers which have unexpectedly ended.

        """
        ...
    
    def create_workers(self): # -> None:
        ...
    
    def close_channels(self): # -> None:
        ...
    
    def stop_workers(self): # -> None:
        ...
    
    def new_worker(self): # -> None:
        ...
    
    def stop_worker(self, worker_id: int, force=...): # -> None:
        ...
    


def worker_process(params: Worker, channel: WorkerChannel): # -> None:
    """The worker process routines."""
    ...

def worker_get_next_task(channel: WorkerChannel, max_tasks: int): # -> Generator[Task, Any, None]:
    ...

def send_result(channel: WorkerChannel, result: Any): # -> None:
    """Send result handling pickling and communication errors."""
    ...

def fetch_task(channel: WorkerChannel) -> Task:
    ...

def task_transaction(channel: WorkerChannel) -> Task:
    """Ensures a task is fetched and acknowledged atomically."""
    ...

def task_worker_lookup(running_tasks: tuple, worker_id: int) -> Task:
    ...

def process_chunk(function: Callable, chunk: list) -> list:
    """Processes a chunk of the iterable passed to map dealing with errors."""
    ...

def interpreter_shutdown(): # -> None:
    ...

def dictionary_values(dictionary: dict) -> tuple:
    """Returns a snapshot of the dictionary values handling race conditions."""
    ...

GLOBAL_SHUTDOWN = ...
WORKERS_NAME = ...
NoMessage = ...
TaskResult = ...
TaskProblem = ...
WorkerTask = ...
Acknowledgement = ...

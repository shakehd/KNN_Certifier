%!TEX root = ../dissertation.tex

\chapter{Related Works}
\label{chp:related-works}

Adversarial robustness regained significant research attention after it was demonstrated that non-linear classifiers, such as support vector machines with the kernel trick or neural networks, are not robust to adversarial examples \cite{biggio2013, biggio_2013, szegedy2014}. Consequently, robustness certification of \acs{ML} models (i.e., the theoretical assurance that a model remains unaffected by perturbations within a specified bound) emerged as a prominent research topic. Several techniques, such as interval bound propagation (IBP) \cite{gowal2019, zhang2019} and abstract interpretation \cite{RanzatoUZ21, ferrara, vechev-sp18, vechev-icml18, MullerS0PV21, RZ19, RZ20, RanzatoZgecco21, vechev-nips18, singh2019, SinghGPV-iclr19, Survey}, have been proposed for certify the robustness. Although the majority of research has focused on neural network-based models, other major \acs{ML} models particularly non-parametric ones like \acs{$k$-NN}, have been only recently studied. Specifically for the $k$-nearest neighbor method, in \cite{WangJC18} the authors, for the first time, studied the robustness property from theoretical point of view and showed that \acs{$k$-NN} classifier can be as robust as the optimal Bayes classifier given a sufficiently large number of samples and $k$. Others have proposed various methodologies based on some minimization problems to find the smallest perturbations of a sample that changes its classification \cite{Wang19, YangRWC20, SitawarinW19, SitawarinW20, SitawarinKSW21} and more recently in \cite{Nicolo-knn} abstract interpretation was used to verify the stability of \acs{$k$-NN} given a perturbation of a test sample like we did in our research work.

This chapter will provide a brief overview of the aforementioned stream of works finding adversarial examples on \acsp{$k$-NN}.

\section {Theoretical analysis}
\label{sec:theory-analysis}
Wang et al. \cite{WangJC18} conducted a theoretical study on the robustness of \acs{$k$-NN} to adversarial attacks. For simplicity, they focused on binary classification and Euclidean distance. Their analysis demonstrated that, given an input sample $\vinput$, if certain continuity assumptions hold within a neighborhood of $\vinput$, then when $k$ is constant (i.e., independent of the training set size $n$ and the input space dimensionality), \acs{$k$-NN}  is inherently non-robust as $n \rightarrow \infty $. This non-robustness is particularly evident in regions where  $p(y=1 | x) \in (0,1)$, indicating areas where samples with different labels are not clearly separated. On the other hand, if $k = \Omega(\sqrt{dn \log n})$ where $d$ is the data dimension and $n$ is the training set size, they demonstrated that, as $n \rightarrow \infty $, the robustness region of $k$-nearest neighbors approaches that of the Bayes Optimal classifier. Since $k = \Omega(\sqrt{dn \log n})$ is not a usable value in practice, they propose a novel method to make $1$-nearest neighbor more robust to adversarial attacks. Following the observation that $1$-nearest neighbor is robust when oppositely labeled points are far apart, their idea is to enforce this property by removing training samples $x$ such that: (a) there is a lack of confidence about the label of $x$ and its nearby points and (2) the closest points of $x$ are not of the same label. After removing this points they executed the $1$-nearest neighbor on the remaining dataset. Their experiments showed that their modified version performs better than or about as well as both standard $1$-nearest neighbors and nearest neighbors with adversarial training.

\section {\acs{QP}-based analysis}
\label{sec:qp-analysis}

In 2019, Wang et al. \cite{Wang19} studied the problem of evaluating the robustness of $k$-nearest neighbor classifiers, focusing more on $1$NNs. They showed that finding the minimum adversarial perturbation can be formulated as a set of convex \emph{quadratic programming} (\emph{\acs{QP}}) problems, with an exact solution in polynomial time for $1$NNs. When applied to general \acs{$k$-NN} models, however, the number of constraints in the \acs{QP} formulation grows exponentially with $k$, becoming quickly infeasible, hence NP-hard. As a result, for $k > 1$, their method finds valid lower and upper bounds of the minimum adversarial perturbation. Given a classifier $C\colon X \rightarrow L$ and an input sample $(x, l) \in X \times L$, an adversarial perturbation is defined as $\vec{d} \in \mathbb{R}^n \textbf{ s.t. } C(\vec{x} + \vec{d}) \neq l$, and is the minimum if $\forall \vec{d}' \in \mathbb{R}^n.\: \lVert \vec{d}' \rVert_2 < \lVert \vec{d} \rVert_2 \Rightarrow C(\vec{x} + \vec{d}') = l$. For instance, let $L = \{A, B\}$, $(x, A) \in X \times L$ and $k = 1$, the problem of finding the minimum perturbation such that $\vec{x} + \vec{d}$ is closer to some $\vec{x_j}$ labeled with $l_j = B$ than to all class-$A$ samples, can be formulated as the quadratic primal problem:
\begin{equation*}
	\epsilon^{(j)} \triangleq \argmin\limits_{\vec{d}} \frac{1}{2}\vec{d}^T\vec{d} \textbf{ s.t. } \lVert \vec{x} + \vec{d} - \vec{x_j} \rVert_2^2 \leq  \lVert \vec{x} + \vec{d} - \vec{x_i} \rVert_2^2, ~~~ \forall i,l_i = A
\end{equation*}
Although it is simple to solve, by scaling to $k = 3$ it become necessary to list all the possible combinations of $\{(j_1, j_2, j_3) ~|~ l_{j_1} = l_{j_2} = B, l_{j_3} = A\}$ and then solve the \acs{QP} primal problem to force $\vec{x} + \vec{d}$ to be closer to $\vec{x_{j_1}}$, $\vec{x_{j_2}}$ than to all class-$A$ samples except $\vec{x_{j_3}}$, hence:
\begin{equation*}
	\epsilon^{(j_1,j_2,j_3)} \triangleq \argmin\limits_{\vec{d}} \frac{1}{2}\vec{d}^T\vec{d} \textbf{ s.t. } \lVert \vec{x} + \vec{d} - \vec{x_j} \rVert_2^2 \leq  \lVert \vec{x} + \vec{d} - \vec{x_i} \rVert_2^2, ~~~ \forall i, i \neq j_3, l_i = A, j \in \{j_1, j_2\}
\end{equation*}

Thus, it is more expensive to solve. For general $k > 1$, the \acs{QP} formulation will have $O(nk)$ constraints, but because of the sparsity of solution, greedy coordinate ascent can still solve a subproblem efficiently, computing an upper and lower bound, corresponding to attack and verification.

\section {Region-based analysis}
\label{sec:region-analysis}

Back in 2020, Yang et al. \cite{YangRWC20} investigated adversarial examples for the most popular non-parametric classifiers, including k-nearest neighbors, decision trees and random forests. They devised a general attack technique, known as \emph{region-based attack}, designed to work well for multiple non-parametrics. Since the difficulty in finding adversarial examples stems from the fact that these classifiers have complicated decision regions, the main idea behind this attack is to decompose the decision regions of many classifiers, such as \acs{$k$-NN} or random forests, into convex sets. Precisely, they leverage the concept of $(s, m)$-decomposition, that is a partition of $\mathbb{R}^n$ into $s$ convex polyhedra $P_1,\dots,P_s$ such that each $P_i$ can be described by up to $m$ linear constraints. They formulated  that a classifier is $C\colon X \rightarrow L$ $(s, m)$-decomposable when there is a $(s, m)$-decomposition such that $C$ is constant on $P_i$ for each $i \in [1,\dots,s]$. In this context, given a classifier with a decomposition  $P_1,\dots,P_s$ such that $C(\vec{y}) = l_i$ when $\vec{y} \in P_i$ for labels $l_i \in L$, finding an adversarial example for an input sample $\vec{x} \in X$ requires outputting $\bar{\vec{x}}$ that minimizes::
\begin{equation*}
	\min\limits_{i\colon C(\vec{x}) \neq l_i} ~~ \min\limits_{\vec{z} \in P_i} ~ \lVert \vec{x} - \vec{z} \rVert_{p \in \{1, 2, \infty\}}
\end{equation*}
Thus, solving the inner minimization problem results in candidates $\vec{z}^i \in P_i$. Taking then the outer minimum over $i$ with $C(\vec{x}) \neq l_i$ leads to the optimal adversarial example:
\begin{equation*}
	\bar{\vec{x}} \triangleq \argmin\limits_{\vec{z}^i} ~ \lVert \vec{x} - \vec{z}^i \rVert_{p \in \{1, 2, \infty\}}
\end{equation*}
The exact attack algorithm's performance is determined by two factors: (i) the number of regions, which is determined by the complexity of the classifier, and (ii) the number of constraints and dimensionality of the polyhedra. For $k$NN classifiers the number of convex polyhedra scales with $O(n^k)$: when $k = 1$, this is efficiently solvable, because polyhedra have at most $n$ constraints and the adversarial examples can be found quickly using a linear program for $\ell_\infty$-perturbations. Unfortunately, for $k > 1$, region-based attacks do not scale well, and an approximation algorithm for larger values of $k$ is also discussed.


\section {Gradient-based analysis}
\label{sec:gradient-analysis}

Inspired by their previous work \cite{SitawarinWDeep19} on the robustness of deep \acs{$k$-NN}, in 2020 Sitawarin and Wagner \cite{SitawarinW20} proposed a new state-of-the-art attack, called \emph{gradient-based attack}, to evaluate the robustness of \acs{$k$-NN} classifiers. Their method also outperforms Yang et al. \cite{YangRWC20} in that when $k > 1$, an adversarial example with a smaller perturbation is found in less than $1\%$ of the running time. Moreover, increasing the value of $k$ only slightly increases the algorithm's running time. In order to find the minimum $\vec{d}^* \in \mathbb{R}^n \textbf{ s.t. } \bar{\vec{x}} \triangleq \vec{x} + \vec{d}^*$ is an adversarial example classified differently than $\vec{x} \in X$, the following minimization problem is solved:
\begin{equation*}
	\vec{d}^* \triangleq \argmin\limits_{\vec{d}} \sum\limits_{i = 1}^{m} \max\left\{w_i\left(\lVert \bar{\vec{x_i}} - (\vec{x} + \vec{d})\rVert_2^2 - \eta^2\right) + \Delta, 0\right\}
\end{equation*}
where $m$ denote the mean of examples with different class closest to $\vec{x}$ in $\ell_2$-norm, $w_i = 1$ if $\bar{\vec{x_i}}$ is adversarial, otherwise $w_i = -1$, and $\eta$ is the distance to the $k$-th nearest neighbor. The changes compared to the original version are in using the rectifier $\max(z \in \mathbb{R}, 0)$ instead of sigmoid, which avoids the need to deal with overflow and underflow issues caused by the exponential in distance computation, and the introduction of a small gap $\Delta$ to ensures that $\bar{\vec{x_i}}$ is a
bit closer to the guide samples from the incorrect class than the ones from the correct class. Unfortunately gradient-based approaches generally do not work well for finding minimum $\ell_\infty$-norm adversarial examples, therefore only $\ell_2$-norm is considered. Furthermore, this method does not guarantee the optimality of the solution, not even for $k = 1$.

\section {Geometric-based analysis}
\label{sec:geometric-analysis}

GeoAdEx \cite{SitawarinKSW21}, which stands for \emph{geometric adversarial example}, is the product of the most recent work finding adversarial examples on \acsp{$k$-NN}. This novel tool was proposed by Sitawarin et al. back in 2021, and is the first using geometric approach to perform a search that expands outwards from the given input sample. The main idea of GeoAdEx is to perform a principled geometric exploration around the test sample by processing order-$k$ Voronoi cells à la breadth-first search until it discovers an adversarial cell. Formally, the goal of this algorithm is to find the smallest perturbation $\vec{d}^* \in \mathbb{R}^n$ that moves a test point $(\vec{x}, l_{\vec{x}}) \in X \times L$, to an adversarial cell, that is an order-$k$ Voronoi cell that has different majority than label $l_{\vec{x}}$. Such objective can be expressed as the following optimization problem:
\begin{equation*}
	\vec{d}^* \triangleq \operatorname*{arg\,min}_{\vec{d}} ~ \lVert \delta \rVert_2^2 ~~~ \textbf{s.t.  } \vec{x} + \vec{d} \in A(x)
\end{equation*}
where $A(x)$ is the set of all adversarial cells with respect to $(\vec{x}, l_{\vec{x}})$ and $\ell_2$-norm. The above constraint implies that $\vec{x} + \vec{d}$ must be a member of an adversarial cell from $A(x)$. A simple approach to solve this minimization is to build a series of optimization problems, one for each of the cells in $A(x)$, and pick the solution with the minimum adversarial distance. Unfortunately, this would require solving $O({n \choose k})$ \acs{QP} problems, each of which has $k(n - k)$ constraints. While this complexity may be manageable when $k = 1$ and $n$ is small, as it was in the other works, it does not scale well with $k$ becoming quickly unusable. To deal with $k > 1$, the authors of this tool have added an approximated version of the algorithm which consists in bounding the number of neighboring cells taken into account according to a fast heuristic. However, the main drawback of this approach is that the approximation may affect the optimality, which is therefore no longer guaranteed. Overall this geometric-based attack finds an adversarial distance that is closer to the optimal than the baselines, but its main limitation is the excessively long runtime of the non-approximated version.

\section {Abstract interpretation-based analysis}
\label{sec:Abstract-analysis}
Fassina et al. \cite{Nicolo-knn} proposed novel formal and automatic verification method, implemented in a tool called kNAVe ($k$\textbf{N}N \textbf{A}bstract \textbf{Ve}rifier), to automatically verify when \acs{$k$-NN} is provably stable for an input sample with respect to a given perturbation. Their approach is based upon the well-established framework of abstract interpretation \cite{cousot21}, a formal verification method used to statically analyze the dynamic behavior of a system. They first developed a theoretical sound approximation of the \acs{$k$-NN} algorithm and the similarity metric $\delta$ used by the classifier that are agnostic with respect to a symbolic numerical abstraction $A$ capable of representing input space properties, namely sets of vectors in $\wp(\mathbb{R}^n)$, and basic numerical operations such as addition, product, and modulus. Given an abstract value $a \in A$ which provides a symbolic over-approximation of an adversarial perturbation $P(x)$ of an input sample $x$ modeled as the $\ell_\infty$-ball of radius $\epsilon$ centered in $x$, this approximate classifier $C_{k,\delta}^A$ returns an over-approximation $\mathcal{O}$ of the set of labels computed by \acs{$k$-NN} for all the samples within $P(x)$. Therefore, if $\mathcal{O}$ is a singleton set consisting of the label predict by \acs{$k$-NN} for $x$ then it can be inferred that \acs{$k$-NN} is provably stable on $x$ for its perturbation $P(x)$. Subsequently, they performed empirical evaluation of the abstract classifier by instantiating it with two numerical abstract domains to approximate the range of numerical features: (1) the interval domain \cite{CC77} in which abstract values are closed real intervals (e.g. $x_i \in [l, u]$) and (2) the zonotope domain \cite{Comba1990AaneAA} where the abstract values are affine forms, a first degree polynomial over symbolic noise variable assumed to lie in the interval $[-1, 1]$ (e.g. $x_i = a_0 + \sum_{j=1}^{k} a_j\epsilon_j$ given $a_j \in \mathbb{R}$ and $\epsilon_j \in [-1, 2]$). In their experiment they showed that \acs{$k$-NN} is a robust classification algorithm since for adversarial perturbations of $\le \pm 2\% $, kNAVe was able to infer for several datasets more than $90\%$ of robustness for $k \in {1, 3, 5, 7}$.

One limitation of kNAVe is that the abstract classifier is not complete, i.e., the over-approximation $\mathcal{O}$ does not always equal the set of labels computed by \acs{$k$-NN} for all samples within $P(x)$. Consequently, there are test samples where the abstract classifier cannot prove stability, even though the input samples are genuinely stable under the perturbation. In contrast, the method proposed in this research is exact—it can prove stability for all inputs that are genuinely stable. As shown in \autoref{chp:experimental-evaluation}, our method outperforms kNAVe across all datasets for the same values of $k$.

\label{sec:abs-inter-ver}

